# Bi-lingual Induction Experiment Log

á€¡á€›á€„á€ºá€†á€¯á€¶á€¸ á€’á€±á€á€¬ á€•á€¼á€„á€ºá€†á€„á€ºá€á€²á€·...  

corpus á€¡á€á€½á€€á€ºá€€ ASEAN-MT á€’á€±á€á€¬á€›á€šá€º (en, th, my), TALPCo á€’á€±á€á€¬á€›á€šá€º (en, th, my), BEST thai corpus á€›á€šá€º, myword+para (for my),

```
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ cp data_eng.txt ../../exp1/en/  
```

```
(base) ye@:/media/ye/SP PHD U3/test-myWord/myWord-main$ python ./myword.py word /home/ye/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus/data_myn.txt /home/ye/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus/data_myn.word.txt
```

á€’á€«á€•á€±á€™á€²á€· myWord á€”á€²á€· á€–á€¼á€á€ºá€á€²á€· output á€‘á€€á€º manual á€–á€¼á€á€ºá€‘á€¬á€¸á€•á€¼á€®á€¸á€á€¬á€¸ á€›á€¾á€­á€á€¬á€™á€­á€¯á€· á€¡á€²á€’á€«á€€á€­á€¯á€•á€² á€šá€°á€á€¯á€¶á€¸á€–á€­á€¯á€· á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€á€²á€·...  

```
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ wget https://raw.githubusercontent.com/matbahasa/TALPCo/master/myn/data_myn-token.txt 
--2021-09-26 04:13:59--  https://raw.githubusercontent.com/matbahasa/TALPCo/master/myn/data_myn-token.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 188242 (184K) [text/plain]
Saving to: â€˜data_myn-token.txtâ€™

data_myn-token.txt                    100%[=========================================================================>] 183.83K  --.-KB/s    in 0.1s    

2021-09-26 04:13:59 (1.51 MB/s) - â€˜data_myn-token.txtâ€™ saved [188242/188242]

(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ head data_myn-token.txt 
1176
á€™á€…á€¹á€…á€á€¬
á€á€¬á€”á€¬á€á€«
á€Ÿá€¬
á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸
á€™
á€Ÿá€¯á€á€º
á€•á€«
á€˜á€°á€¸
á‹
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ 
```

column to line conversion:  

```
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ perl ./col2line.pl ./data_myn-token.txt > ./data_myn-token.txt.line
```

```
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ head ./data_myn-token.txt.line 
1176 á€™á€…á€¹á€…á€á€¬ á€á€¬á€”á€¬á€á€« á€Ÿá€¬ á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸ á€™ á€Ÿá€¯á€á€º á€•á€« á€˜á€°á€¸ á‹
1178 á€¡á€–á€± á€€ á€€á€»á€±á€¬á€„á€ºá€¸á€†á€›á€¬ á€•á€« á‹
1180 á€€á€»á€±á€¬á€„á€ºá€¸ á€•á€­á€á€º á€á€šá€º á‹
1194 á€á€­á€¯á€€á€»á€­á€¯ á€™á€¾á€¬ á€”á€± á€á€¬ á€á€šá€º á‹
1222 á€•á€”á€ºá€¸á€á€¼á€¶ á€™á€¾á€¬ á€á€…á€ºá€•á€„á€º á€á€½á€± á€›á€¾á€­ á€á€šá€º á‹
1229 á€™á€…á€¹á€…á€á€¬ á€á€¬á€”á€¬á€á€« á€˜á€šá€º á€™á€¾á€¬ á€›á€¾á€­ á€ á€œá€² á‹
1233 á€•á€­á€¯á€€á€ºá€†á€¶ á€™ á€›á€¾á€­ á€˜á€°á€¸ á‹
1244 á€…á€¬á€›á€±á€¸á€á€¯á€¶ á€•á€±á€«á€º á€™á€¾á€¬ á€…á€¬á€¡á€¯á€•á€º á€›á€¾á€­ á€á€šá€º á‹
1245 á€…á€¬á€›á€±á€¸á€á€¯á€¶ á€¡á€±á€¬á€€á€º á€™á€¾á€¬ á€œá€½á€šá€ºá€¡á€­á€á€º á€›á€¾á€­ á€á€šá€º á‹
1246 á€¡á€­á€á€º á€‘á€² á€™á€¾á€¬ á€™á€¾á€á€ºá€…á€¯á€…á€¬á€¡á€¯á€•á€º á€›á€¾á€­ á€á€šá€º á‹
```

Remove line no:  

```
$ sed 's/^ *[0-9]\+.//g' ./data_myn-token.txt.line > ./data_myn-token.txt.line.rm-lineno 
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ head ./data_myn-token.txt.line.rm-lineno 
á€™á€…á€¹á€…á€á€¬ á€á€¬á€”á€¬á€á€« á€Ÿá€¬ á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸ á€™ á€Ÿá€¯á€á€º á€•á€« á€˜á€°á€¸ á‹
á€¡á€–á€± á€€ á€€á€»á€±á€¬á€„á€ºá€¸á€†á€›á€¬ á€•á€« á‹
á€€á€»á€±á€¬á€„á€ºá€¸ á€•á€­á€á€º á€á€šá€º á‹
á€á€­á€¯á€€á€»á€­á€¯ á€™á€¾á€¬ á€”á€± á€á€¬ á€á€šá€º á‹
á€•á€”á€ºá€¸á€á€¼á€¶ á€™á€¾á€¬ á€á€…á€ºá€•á€„á€º á€á€½á€± á€›á€¾á€­ á€á€šá€º á‹
á€™á€…á€¹á€…á€á€¬ á€á€¬á€”á€¬á€á€« á€˜á€šá€º á€™á€¾á€¬ á€›á€¾á€­ á€ á€œá€² á‹
á€•á€­á€¯á€€á€ºá€†á€¶ á€™ á€›á€¾á€­ á€˜á€°á€¸ á‹
á€…á€¬á€›á€±á€¸á€á€¯á€¶ á€•á€±á€«á€º á€™á€¾á€¬ á€…á€¬á€¡á€¯á€•á€º á€›á€¾á€­ á€á€šá€º á‹
á€…á€¬á€›á€±á€¸á€á€¯á€¶ á€¡á€±á€¬á€€á€º á€™á€¾á€¬ á€œá€½á€šá€ºá€¡á€­á€á€º á€›á€¾á€­ á€á€šá€º á‹
á€¡á€­á€á€º á€‘á€² á€™á€¾á€¬ á€™á€¾á€á€ºá€…á€¯á€…á€¬á€¡á€¯á€•á€º á€›á€¾á€­ á€á€šá€º á‹
```

á€…á€¬á€œá€¯á€¶á€¸á€–á€¼á€á€ºá€•á€¼á€®á€¸ á€€á€±á€¬á€ºá€œá€¶á€¡á€œá€­á€¯á€€á€ºá€–á€¼á€…á€ºá€”á€±á€á€²á€· á€‘á€­á€¯á€„á€ºá€¸ á€–á€­á€¯á€„á€ºá€€á€­á€¯á€œá€Šá€ºá€¸ á€•á€¯á€¶á€™á€¾á€”á€º left-to-right line á€á€½á€±á€¡á€–á€¼á€…á€º á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€á€²á€·...  

```
$ perl ./col2line.pl ./data_tha-token.txt > ./data_tha-token.txt.line 
$ sed 's/^ *[0-9]\+.//g' ./data_tha-token.txt.line > ./data_tha-token.txt.line.rm-lineno

(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings/th-data/TALPCo-parallel-corpus$ head ./data_tha-token.txt.line.rm-lineno 
à¸„à¸¸à¸“ à¸—à¸²à¸™à¸²à¸à¸° à¹„à¸¡à¹ˆ à¹ƒà¸Šà¹ˆ à¸™à¸±à¸ à¹€à¸£à¸µà¸¢à¸™ à¸„à¸£à¸±à¸š
à¸à¹ˆà¸­ à¹€à¸›à¹‡à¸™ à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ à¸„à¸£à¸±à¸š
à¹‚à¸£à¸‡ à¹€à¸£à¸µà¸¢à¸™ à¸«à¸¢à¸¸à¸” à¸„à¸£à¸±à¸š
à¹‚à¸•à¹€à¸à¸µà¸¢à¸§ à¸­à¸²à¸à¸²à¸¨ à¹à¸ˆà¹ˆà¸¡à¹ƒà¸ª à¸„à¸£à¸±à¸š
à¸—à¸µà¹ˆ à¸ªà¸§à¸™ à¸ªà¸²à¸˜à¸²à¸£à¸“à¸° à¸¡à¸µ à¸•à¹‰à¸™ à¹„à¸¡à¹‰ à¸„à¸£à¸±à¸š
à¸„à¸¸à¸“ à¸—à¸²à¸™à¸²à¸à¸° à¸­à¸¢à¸¹à¹ˆ à¸—à¸µà¹ˆ à¹„à¸«à¸™ à¸„à¸£à¸±à¸š
à¹„à¸¡à¹ˆ à¸¡à¸µ à¹€à¸‡à¸´à¸™ à¸„à¸£à¸±à¸š
à¸šà¸™ à¹‚à¸•à¹Šà¸° à¸¡à¸µ à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­ à¸„à¸£à¸±à¸š
à¹ƒà¸•à¹‰ à¹‚à¸•à¹Šà¸° à¸¡à¸µ à¸à¸£à¸°à¹€à¸›à¹‹à¸² à¸„à¸£à¸±à¸š
à¹ƒà¸™ à¸à¸£à¸°à¹€à¸›à¹‹à¸² à¸¡à¸µ à¸ªà¸¡à¸¸à¸” à¹‚à¸™à¹‰à¸• à¸„à¸£à¸±à¸š
```

## Downloading UMBC English Corpus

á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€€á€­á€¯á€œá€Šá€ºá€¸ á€€á€­á€¯á€šá€ºá€·á€…á€€á€ºá€‘á€²á€™á€¾á€¬ word2vec á€”á€²á€· fasttext á€™á€±á€¬á€ºá€’á€šá€ºá€€á€­á€¯ á€†á€±á€¬á€€á€ºá€™á€šá€ºá€†á€­á€¯á€›á€„á€º download á€œá€¯á€•á€ºá€–á€­á€¯á€· á€œá€­á€¯á€¡á€•á€ºá€á€šá€ºá‹  

```
(base) ye@:/media/ye/project2/data/umbc-en$ wc UMBC_tokenized_full.txt 
    40599164  3337113760 18786225225 UMBC_tokenized_full.txt
```

```
(base) ye@:/media/ye/project2/data/umbc-en$ ll -h  UMBC_tokenized_full.txt 
-rwxr-xr-x 1 ye ye 18G á€…á€€á€º   26 14:45 UMBC_tokenized_full.txt*
```

```
(base) ye@:/media/ye/project2/data/umbc-en$ head ./UMBC_tokenized_full.txt 
Caucasian skin color on the penis can vary , with the glans being darker than the shaft of the penis , primarily due to increased surface vascularity . Scarring and inflammation can leave less pigmented areas on the penile skin which can look like skin color variation . 
If what you are noticing is new to you , it bears an exam to confirm that you do not have a skin infection causing the change . If it has always been that way for you since adolescence , then this is simply your coloration and it is normal . 
This has only happened in the last 3 weeks or so . The end of my penis ( the helmet ) is a white/blue colour . Even when I get an erection the end is still white/blue . On one side it is very rough . The rest of the head goes a normal blood filled red apart from these 2 patches either side . Also my foreskin seems to be a little tighter than usual . 
The following courses are required as the Core for concentrations with a Law , Diversity and Justice emphasis . This Core , combined with other courses through the Fairhaven concentration process and close faculty advisement , provides the basis for varied paths of study exploring the issues of law , diversity and justice in our society . 
This interdisciplinary seminar is an introduction to modern social theory . It employs critical social theories to explore social relationships and examine society from positions of race , class , gender , and sexuality , focusing specifically on the rights , responsibilities and obligations of individuals and communities . Integral to this examination are the experiences of those excluded from the Western ideals of freedom and equality that , arguably , form the basis of liberal democracy . This seminar must be taken either the first quarter or second quarter of enrollment at Fairhaven . 
Study of the American legal system and how it affects individuals and society , the structure and evolving nature of the legal system , and legal reasoning and the role of courts in government . Skill development in reading and analyzing court opinions . 
Study of American ideas of rights and liberties , what they mean in practice , competing principles and ideologies at work in the arena of constitutional rights , the history of our justice system with regard to rights and liberties and an exploration of where it is currently headed . 
Through the Concentration process and in close consultation with faculty advisors , students will add courses to the Core and Focus courses from the Fairhaven and WWU course catalogues to develop a concentration tailored to their career or graduate education goals . Faculty advisors will provide detailed information to their advisees about the courses available to them . 
`` I strongly believe that the underlying structures and philosophy of Fairhaven College , the abilities of my professors and the support they provided , coupled with my own interests and approach to learning were critical components to what made college such a positive experience for me . 
The knowledge and skills of my professors , and their manner of communicating , of questioning , and of teaching has many times served significantly as inspiration and motivation for me to learn more , to push beyond my supposed boundaries , and most importantly , to feel able to make mistakes . I feel very lucky to have attended Fairhaven College and for the education I gained from it . '' 
```

```
(base) ye@:/media/ye/project2/data/umbc-en$ tail ./UMBC_tokenized_full.txt 
Whither went Tammuz ? His destination has already been referred to as `` the bosom of the earth '' , and in the Assyrian version of the `` Descent of Ishtar '' he dwells in `` the house of darkness '' among the dead , `` where dust is their nourishment and their food mud '' , and `` the light is never seen '' -- the gloomy Babylonian Hades . In one of the Sumerian hymns , however , it is stated that Tammuz `` upon the flood was cast out '' . The reference may be to the submarine `` house of Ea '' , or the Blessed Island to which the Babylonian Noah was carried . In this Hades bloomed the nether `` garden of Adonis '' . 
`` I am Sargon , the mighty King of Akkad . My mother was a vestal ( priestess ) , my father an alien , whose brother inhabited the mountain . . . . When my mother had conceived me , she bare me in a hidden place . She laid me in a vessel of rushes , stopped the door thereof with pitch , and cast me adrift on the river . . . . The river floated me to Akki , the water drawer , who , in drawing water , drew me forth . Akki , the water drawer , educated me as his son , and made me his gardener . As a gardener , I was beloved by the goddess Ishtar . '' 
It is unlikely that this story was invented by Sargon . Like the many variants of it found in other countries , it was probably founded on a form of the Tammuz-Adonis myth . Indeed , a new myth would not have suited Sargon 's purpose so well as the adaptation of an old one , which was more likely to make popular appeal when connected with his name . The references to the goddess Ishtar , and Sargon 's early life as a gardener , suggest that the king desired to be remembered as an agricultural Patriarch , if not of divine , at any rate of semi-divine origin . 
Heimdal , watchman of the Teutonic gods , also dwelt for a time among men as `` Rig '' , and had human offspring , his son Thrall being the ancestor of the Thralls , his son Churl of churls , and Jarl of noblemen . 
I spread like a bird my hands . I descend , I descend to the house of darkness , the dwelling of the god Irkalla : To the house out of which there is no exit , To the road from which there is no return : To the house from whose entrance the light is taken , The place where dust is their nourishment and their food mud . Its chiefs also are like birds covered with feathers ; The light is never seen , in darkness they dwell . . . . 
Keeper of the waters , open thy gate , Open thy gate that I may enter . If thou openest not the gate that I may enter I will strike the door , the bolts I will shatter , p. 96 I will strike the threshold and will pass through the doors ; I will raise up the dead to devour the living , Above the living the dead shall exceed in numbers . 
Let me weep over the strong who have left their wives , Let me weep over the handmaidens who have lost the embraces of their husbands , Over the only son let me mourn , who ere his days are come is taken away . 
May I imprison thee in the great prison , May the garbage of the foundations of the city be thy food , May the drains of the city be thy drink , May the darkness of the dungeon be thy dwelling , May the stake be thy seat , May hunger and thirst strike thy offspring . 
Since thou hast not paid a ransom for thy deliverance to her ( Allatu ) , so to her again turn back , For Tammuz the husband of thy youth . The glistening waters ( of life ) pour over him . In splendid clothing dress him , with a ring of crystal adorn him . 
A Sumerian hymn to Tammuz throws light on this narrative . It sets forth that Ishtar descended to Hades to entreat him to be glad and to resume care of his flocks , but Tammuz refused or was unable to return . 
(base) ye@:/media/ye/project2/data/umbc-en$ 
```

--------------

filesize á€á€½á€±á€€ á€¡á€›á€™á€ºá€¸á€€á€¼á€®á€¸á€œá€­á€¯á€· á€”á€±á€¬á€€á€º á€™á€±á€¬á€ºá€’á€šá€ºá€á€½á€± á€‘á€½á€€á€ºá€œá€¬á€á€²á€· á€¡á€á€«á€™á€¾á€¬á€œá€Šá€ºá€¸ á€”á€±á€›á€¬á€šá€°á€”á€­á€¯á€„á€ºá€œá€­á€¯á€· á€–á€­á€¯á€„á€ºá€á€½á€±á€€á€­á€¯ project2/ HDD á€¡á€±á€¬á€€á€ºá€€á€­á€¯ á€›á€½á€¾á€±á€·á€œá€­á€¯á€€á€ºá€á€šá€ºá‹  

á€™á€¼á€”á€ºá€™á€¬á€…á€¬ corpus á€€ á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸...   

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my$ cat corpus2-and-para 3_all.my.word data_myn-token.txt.line.rm-lineno > my_corpus.txt
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my$ wc my_corpus.txt 
   633956  13158666 170988793 my_corpus.txt
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my$ 
```


## Building word2vec and fasttext models

shell script á€›á€±á€¸á€á€²á€·...  

```bash
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ cat ./mk-word2vec-fasttext.sh 
#!/bin/bash

# STEP No. 1: building word2vec and fasttext models
#
# written by Ye Kyaw Thu, LST, NECTEC, Thailand
# Date 25 Sept 2021
# How to run: ./mk-word2vec-fasttext.sh <source-corpus>  <src-output-folder> <target-corpus> <trg-output-folder>
# e.g. ./mk-word2vec-fasttext.sh /media/ye/project2/exp/bilingual-induction/exp1/my/my_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/my/ /media/ye/project2/data/umbc-en/UMBC_tokenized_full.txt /media/ye/project2/exp/bilingual-induction/exp1/en/ 2>&1 | tee my-en.exp1.log
#./mk-word2vec-fasttext.sh /media/ye/project2/exp/bilingual-induction/exp1/my/my_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/my/ /media/ye/project2/exp/bilingual-induction/exp1/th/th_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/th/ 2>&1 | tee my-th.exp1.log


echo "change python environment...";
#conda activate bilingual-emb

# Building word2vec
# python3 -i src/train_embeddings.py --corpus YOUR-CORPUS --model YOUR-MODEL --output-directory YOUR-OUTPUT-DIR

# for source
echo "start building a word2vec model for SRC language ...  ";
#time python3 src/train_embeddings.py --corpus $1 --model word2vec --output-directory $2

# for target
echo "start building a word2vec model for TRG language ...  ";
time python3 src/train_embeddings.py --corpus $3 --model word2vec --output-directory $4
 
 # Building fasttext
echo "building a fasttext model for SRC language..."
#time python3 src/train_embeddings.py --corpus $1 --model fasttext --output-directory $2;

echo "building a fasttext model for TRG language..."
time python3 src/train_embeddings.py --corpus $3 --model fasttext --output-directory $4;

echo "check for SRC:  see word2vec and fastext models...";
 ls $2;
echo "check for TRG:  see word2vec and fastext models...";
 ls $4;


(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$
```

Run above shell script...  

```
(bilingual-emb)  ye@~/tool/en-cy-bilingual-embeddings$ ./mk-word2vec-fasttext.sh /media/ye/project2/exp/bilingual-induction/exp1/my/my_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/my/ /media/ye/project2/data/umbc-en/UMBC_tokenized_full.txt /media/ye/project2/exp/bilingual-induction/exp1/en/
...
...
...
2021-09-26 15:58:31,102 : INFO : PROGRESS: at sentence #10570000, processed 50822549 words, keeping 39505036 word types
2021-09-26 15:58:31,445 : INFO : PROGRESS: at sentence #10580000, processed 50866439 words, keeping 39538482 word types
2021-09-26 15:58:31,928 : INFO : PROGRESS: at sentence #10590000, processed 50910957 words, keeping 39572544 word types
2021-09-26 15:58:32,726 : INFO : PROGRESS: at sentence #10600000, processed 50954727 words, keeping 39606533 word types
./mk-word2vec-fasttext.sh: line 23: 172990 Killed                  python3 src/train_embeddings.py --corpus $3 --model word2vec --output-directory $4

real	6m7.060s
user	4m42.179s
sys	0m15.686s
building a fasttext model for SRC language...
Loading corpus: /media/ye/project2/exp/bilingual-induction/exp1/my/my_corpus.txt
2021-09-26 15:59:52,598 : INFO : resetting layer weights
./mk-word2vec-fasttext.sh: line 27: 173138 Killed                  python3 src/train_embeddings.py --corpus $1 --model fasttext --output-directory $2

real	0m12.280s
user	0m4.140s
sys	0m8.344s
building a fasttext model for TRG language...
Loading corpus: /media/ye/project2/data/umbc-en/UMBC_tokenized_full.txt
2021-09-26 16:00:04,857 : INFO : resetting layer weights
./mk-word2vec-fasttext.sh: line 30: 173169 Killed                  python3 src/train_embeddings.py --corpus $3 --model fasttext --output-directory $4

real	0m6.203s
user	0m4.110s
sys	0m2.202s
check for SRC:  see word2vec and fastext models...
 3_all.my.word	 corpus2-and-para   data_myn-token.txt.line.rm-lineno   my_corpus.txt  'my_corpus.txt_model=word2vec_vectors.vec'
check for TRG:  see word2vec and fastext models...
1_all.en.word  data_eng.txt
```

á€¡á€‘á€€á€ºá€™á€¾á€¬ á€™á€¼á€„á€ºá€›á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸á€•á€² memory á€™á€”á€­á€¯á€„á€ºá€á€¬á€€á€¼á€±á€¬á€„á€ºá€·á€œá€¬á€¸?! killed á€–á€¼á€…á€ºá€€á€¯á€”á€ºá€á€šá€ºá‹
á€™á€¼á€”á€ºá€™á€¬á€…á€¬á€¡á€á€½á€€á€º word2vec model á€á€±á€¬á€· á€†á€±á€¬á€€á€ºá€á€¬ á€•á€¼á€®á€¸á€á€½á€¬á€¸á€á€šá€ºá‹
á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€¡á€á€½á€€á€º word2vec á€™á€†á€±á€¬á€€á€ºá€”á€­á€¯á€„á€ºá€á€²á€·...
á€•á€¼á€®á€¸á€á€±á€¬á€· á€™á€¼á€”á€ºá€™á€¬á€…á€¬á€¡á€á€½á€€á€º fasttext á€™á€†á€±á€¬á€€á€ºá€”á€­á€¯á€„á€ºá€á€²á€·...
á€•á€¼á€®á€¸á€á€±á€¬á€· á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€¡á€á€½á€€á€º fasttext á€™á€±á€¬á€ºá€’á€šá€ºá€œá€Šá€ºá€¸ á€™á€†á€±á€¬á€€á€ºá€”á€­á€¯á€„á€ºá€á€²á€·á€á€¬á€€á€­á€¯ á€á€½á€±á€·á€›...  

coding á€€á€­á€¯ á€…á€…á€ºá€›á€„á€ºá€¸á€”á€²á€· á€¡á€±á€¬á€€á€ºá€•á€« á€…á€¬á€€á€¼á€±á€¬á€„á€ºá€¸á€€á€­á€¯ á€•á€¼á€„á€ºá€›á€™á€šá€ºá€†á€­á€¯á€á€¬á€œá€Šá€ºá€¸ á€á€½á€¬á€¸á€á€½á€±á€·...  

	corpus = data_manager.ExampleCorpus(args.corpus, sep=', ')
	
sep=' ' á€–á€¼á€…á€ºá€›á€™á€šá€º á€™á€¼á€”á€ºá€™á€¬á€…á€¬á€¡á€á€½á€€á€ºá€œá€Šá€ºá€¸...  

## Trying with Thai

á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬ á€’á€±á€á€¬ á€€ text á€á€„á€ºá€•á€² 17GB á€šá€°á€á€±á€¬á€· á€œá€¯á€•á€ºá€™á€šá€ºá€†á€­á€¯á€›á€„á€º á€†á€±á€¬á€€á€ºá€•á€¼á€®á€¸á€á€¬á€¸ word2vec á€€á€­á€¯á€•á€² á€á€¯á€¶á€¸á€œá€­á€¯á€€á€ºá€á€±á€¬á€·á€™á€šá€º..  
á€‘á€­á€¯á€„á€ºá€¸ corpus á€’á€±á€á€¬á€”á€²á€· run á€€á€¼á€Šá€ºá€·á€á€²á€·...  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ ./mk-word2vec-fasttext.sh /media/ye/project2/exp/bilingual-induction/exp1/my/my_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/my/ /media/ye/project2/exp/bilingual-induction/exp1/th/th_corpus.txt /media/ye/project2/exp/bilingual-induction/exp1/th/ 2>&1 | tee my-th.exp1.log
...
...
...
2021-09-26 22:19:25,736 : INFO : EPOCH 10 - PROGRESS: at 40.37% examples, 333182 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:26,748 : INFO : EPOCH 10 - PROGRESS: at 46.76% examples, 332569 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:27,772 : INFO : EPOCH 10 - PROGRESS: at 51.86% examples, 331631 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:28,784 : INFO : EPOCH 10 - PROGRESS: at 56.06% examples, 330183 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:29,788 : INFO : EPOCH 10 - PROGRESS: at 63.62% examples, 333872 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:30,796 : INFO : EPOCH 10 - PROGRESS: at 74.36% examples, 340404 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:31,798 : INFO : EPOCH 10 - PROGRESS: at 83.27% examples, 345196 words/s, in_qsize 5, out_qsize 0
2021-09-26 22:19:32,445 : INFO : worker thread finished; awaiting finish of 2 more threads
2021-09-26 22:19:32,457 : INFO : worker thread finished; awaiting finish of 1 more threads
2021-09-26 22:19:32,467 : INFO : worker thread finished; awaiting finish of 0 more threads
2021-09-26 22:19:32,467 : INFO : EPOCH - 10 : training on 5230564 raw words (4125830 effective words) took 11.8s, 349305 effective words/s
2021-09-26 22:19:32,467 : INFO : training on a 52305640 raw words (41259675 effective words) took 119.1s, 346492 effective words/s
2021-09-26 22:19:33,229 : INFO : storing 28721x300 projection weights into /media/ye/project2/exp/bilingual-induction/exp1/th/th_corpus.txt_model=fasttext_vectors.vec
Loading corpus: /media/ye/project2/exp/bilingual-induction/exp1/th/th_corpus.txt
Embeddings saved to /media/ye/project2/exp/bilingual-induction/exp1/th/th_corpus.txt_model=fasttext_vectors.vec

real	2m28.727s
user	7m12.536s
sys	0m5.877s
check for SRC:  see word2vec and fastext models...
3_all.my.word
corpus2-and-para
corpus2-and-para_model=fasttext_vectors.vec
data_myn-token.txt.line.rm-lineno
my_corpus.txt
my_corpus.txt_model=word2vec_vectors.vec
check for TRG:  see word2vec and fastext models...
2_all.th.word
best.clean.corpus
data_tha-token.txt.line.rm-lineno
th_corpus.txt
th_corpus.txt_model=fasttext_vectors.vec
th_corpus.txt_model=word2vec_vectors.vec
```

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/th$ ll -h *
-rwxr-xr-x 1 ye ye 2.1M á€…á€€á€º   26 03:33  2_all.th.word*
-rwxr-xr-x 1 ye ye  62M á€…á€€á€º   26 03:31  best.clean.corpus*
-rwxr-xr-x 1 ye ye 130K á€…á€€á€º   26 04:36  data_tha-token.txt.line.rm-lineno*
-rwxr-xr-x 1 ye ye  64M á€…á€€á€º   26 21:23  th_corpus.txt*
-rwxr-xr-x 1 ye ye  95M á€…á€€á€º   26 22:19 'th_corpus.txt_model=fasttext_vectors.vec'*
-rwxr-xr-x 1 ye ye  99M á€…á€€á€º   26 22:17 'th_corpus.txt_model=word2vec_vectors.vec'*
```

## Do next Step for my-th

á€™á€¼á€”á€ºá€™á€¬-á€‘á€­á€¯á€„á€ºá€¸ run á€–á€­á€¯á€·á€†á€­á€¯á€›á€„á€º dictionary á€œá€Šá€ºá€¸ á€œá€­á€¯á€¡á€•á€ºá€á€šá€ºá‹  
á€¡á€²á€’á€«á€€á€¼á€±á€¬á€„á€ºá€· á€¡á€›á€„á€ºá€†á€¯á€¶á€¸ á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€º-á€‘á€­á€¯á€„á€ºá€¸ á€¡á€˜á€­á€“á€¬á€”á€ºá€€á€”á€± á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€œá€¯á€¶á€¸á€á€½á€±á€€á€­á€¯á€•á€² á€–á€¼á€á€ºá€‘á€¯á€á€ºá€œá€­á€¯á€€á€ºá€•á€¼á€®á€¸á€á€±á€¬á€· google translate á€”á€²á€· manual á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€á€­á€¯á€„á€ºá€¸á€á€²á€·...  
copy/paste English --> google translate into Myanmar ---> copy/paste á€¡á€œá€¯á€•á€ºá€€á€­á€¯ á€á€›á€€á€º á€œá€±á€¬á€€á€º á€œá€¯á€•á€ºá€œá€­á€¯á€€á€ºá€›...   
á€•á€¼á€®á€¸á€á€½á€¬á€¸á€á€²á€·á€¡á€á€«á€™á€¾á€¬ English-Myanmar dictionary á€›á€œá€¬á€á€²á€·...   

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ paste ./en-th.dict.f1 ./en2my.google.txt > en-my.raw1
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ wc en-my.raw1
 125530  326180 5231117 en-my.raw1
```

á€á€…á€ºá€á€¯á€›á€¾á€­á€á€¬á€€ á€¡á€²á€’á€®á€¡á€‘á€²á€™á€¾á€¬ á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€œá€¯á€¶á€¸á€á€½á€±á€€á€­á€¯ á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€™á€•á€±á€¸á€”á€­á€¯á€„á€ºá€œá€­á€¯á€· input á€‘á€Šá€ºá€·á€œá€­á€¯á€€á€ºá€á€²á€· á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€…á€¬á€œá€¯á€¶á€¸á€¡á€á€­á€¯á€„á€ºá€¸á€•á€² á€‘á€½á€€á€ºá€œá€¬á€á€¬á€á€½á€±á€œá€Šá€ºá€¸ á€›á€¾á€­á€œá€­á€¯á€·  
á€¡á€²á€’á€®á€œá€­á€¯ á€–á€¼á€…á€ºá€”á€±á€á€²á€· pair á€á€½á€±á€€á€­á€¯ á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ awk á€”á€²á€· remove á€œá€¯á€•á€ºá€œá€­á€¯á€·á€›á€á€šá€º...    

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ awk '$1 == $2 { next } { print }' ./test.txt 
abstinent	á€‘á€„á€ºá€›á€¾á€¬á€¸á€á€±á€¬
aka	(á€á€±á€«á€º)
halt	á€›á€•á€ºá€”á€¬á€¸
halt	á€›á€•á€ºá€”á€¬á€¸
ham	á€á€€á€ºá€•á€±á€«á€„á€ºá€á€¼á€±á€¬á€€á€º
ham	á€á€€á€ºá€•á€±á€«á€„á€ºá€á€¼á€±á€¬á€€á€º
handwrite	á€œá€€á€ºá€›á€±á€¸
handwriting	á€œá€€á€ºá€›á€±á€¸
handwritten	á€œá€€á€ºá€›á€±á€¸
anesthesiologist	á€™á€±á€·á€†á€±á€¸á€†á€›á€¬á€á€”á€º
haughtily	á€™á€¬á€”á€€á€¼á€®á€¸
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$
```

á€”á€±á€¬á€€á€ºá€á€…á€ºá€á€¯á€€ uniq á€œá€¯á€•á€ºá€–á€­á€¯á€·á€œá€Šá€ºá€¸ á€œá€­á€¯á€¡á€•á€ºá€á€šá€ºá‹  
shell script á€¡á€–á€¼á€…á€º á€›á€±á€¸á€á€²á€·...  

```bash
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ cat rm-same-column-and-uniq.sh 
#!/bin/bash

# written by Ye, LST, NECTEC, Thailand
# removing if column1 and column2 are the same. make a uniq file.
# Date: 27 Sept 2021
# How to run: ./rm-same-column-and-uniq.sh <input-file>


inputFile=$1;

awk '$1 == $2 { next } { print }' $inputFile > tmp.out
sort ./tmp.out | uniq > $inputFile.clean

rm tmp.out;
#cat $inputFile.clean
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$
```

test file as follows:   

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ cat ./test.txt
Aberdeen	Aberdeen
abstinent	á€‘á€„á€ºá€›á€¾á€¬á€¸á€á€±á€¬
AJAX	AJAX
aka	(á€á€±á€«á€º)
halt	á€›á€•á€ºá€”á€¬á€¸
halt	á€›á€•á€ºá€”á€¬á€¸
ham	á€á€€á€ºá€•á€±á€«á€„á€ºá€á€¼á€±á€¬á€€á€º
ham	á€á€€á€ºá€•á€±á€«á€„á€ºá€á€¼á€±á€¬á€€á€º
handwrite	á€œá€€á€ºá€›á€±á€¸
handwriting	á€œá€€á€ºá€›á€±á€¸
handwritten	á€œá€€á€ºá€›á€±á€¸
anagram	anagram
anchovy	anchovy
anesthesiologist	á€™á€±á€·á€†á€±á€¸á€†á€›á€¬á€á€”á€º
haughtily	á€™á€¬á€”á€€á€¼á€®á€¸
```

test run...  

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ ./rm-same-column-and-uniq.sh ./test.txt
abstinent	á€‘á€„á€ºá€›á€¾á€¬á€¸á€á€±á€¬
aka	(á€á€±á€«á€º)
anesthesiologist	á€™á€±á€·á€†á€±á€¸á€†á€›á€¬á€á€”á€º
halt	á€›á€•á€ºá€”á€¬á€¸
ham	á€á€€á€ºá€•á€±á€«á€„á€ºá€á€¼á€±á€¬á€€á€º
handwrite	á€œá€€á€ºá€›á€±á€¸
handwriting	á€œá€€á€ºá€›á€±á€¸
handwritten	á€œá€€á€ºá€›á€±á€¸
haughtily	á€™á€¬á€”á€€á€¼á€®á€¸
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$
```

running for the en-my dictionary:   

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ time ./rm-same-column-and-uniq.sh ./en-my.raw1

real	0m0.221s
user	0m0.208s
sys	0m0.025s
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ wc ./en-my.raw1
 125530  326180 5231117 ./en-my.raw1
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ wc ./en-my.raw1.clean 
  57326  173067 2977748 ./en-my.raw1.clean
```

## Prepare my-th dictionary

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ paste ./en2my.google.txt ./en-th.dict.f2 > ./my-th.raw1
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ head ./my-th.raw1 
cheese á€¡á€€á€¼á€®á€¸á€€á€¼á€®á€¸	à¹€à¸›à¹‡à¸™à¸ªà¸³à¸™à¸§à¸™à¹à¸›à¸¥à¸§à¹ˆà¸² à¸„à¸™à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸
cart la carte	|à¸¡à¸µà¸—à¸µà¹ˆà¸¡à¸²à¸ˆà¸²à¸à¸ à¸²à¸©à¸²à¸à¸£à¸±à¹ˆà¸‡à¹€à¸¨à¸ª| à¸à¸²à¸£à¸ªà¸±à¹ˆà¸‡à¸­à¸²à¸«à¸²à¸£à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¹€à¸¡à¸™à¸¹à¹„à¸”à¹‰à¸•à¸²à¸¡à¹ƒà¸ˆà¸Šà¸­à¸š à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ªà¸±à¹ˆà¸‡à¹€à¸›à¹‡à¸™à¸Šà¸¸à¸”à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸£à¹‰à¸²à¸™à¸­à¸²à¸«à¸²à¸£à¸ˆà¸±à¸”à¹„à¸§à¹‰ (à¸—à¸²à¸‡à¸›à¸£à¸°à¹€à¸—à¸¨à¹à¸–à¸šà¸•à¸°à¸§à¸±à¸™à¸•à¸ à¸¡à¸±à¸à¸¡à¸µà¸£à¸²à¸¢à¸à¸²à¸£à¸­à¸²à¸«à¸²à¸£à¹€à¸›à¹‡à¸™à¸Šà¸¸à¸”à¸ˆà¸±à¸”à¹„à¸§à¹‰à¹ƒà¸«à¹‰), à¹€à¸¡à¸™à¸¹à¸«à¸£à¸·à¸­à¸£à¸²à¸¢à¸à¸²à¸£à¸­à¸²à¸«à¸²à¸£à¸—à¸µà¹ˆà¸¡à¸µà¸£à¸²à¸„à¸²à¹à¸¢à¸à¹€à¸‰à¸à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸­à¸²à¸«à¸²à¸£à¹à¸•à¹ˆà¸¥à¸°à¸ˆà¸²à¸™
a la carte á€•á€«	à¸”à¸¹ Ã  la carte (à¸ à¸²à¸©à¸²à¸à¸£à¸±à¹ˆà¸‡à¹€à¸¨à¸ª), Related: S. Ã  la carte , 
á€á€”á€ºá€á€…á€ºá€á€¯	à¸¡à¸²à¸, à¸¡à¸²à¸à¸¡à¸²à¸¢ à¹€à¸Šà¹ˆà¸™ a load of rubbish, What a load of muggles.
(á€á€…á€ºá€šá€±á€¬á€€á€º) á€›á€²á€·á€…á€­á€á€ºá€á€…á€ºá€•á€­á€¯á€„á€ºá€¸	à¸„à¸³à¸§à¸´à¸ˆà¸²à¸£à¸“à¹Œà¸«à¸£à¸·à¸­à¸•à¸³à¸«à¸™à¸´à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¸£à¸‡à¹„à¸›à¸•à¸£à¸‡à¸¡à¸²à¹à¸¥à¸°à¸£à¸¸à¸™à¹à¸£à¸‡, à¹€à¸Šà¹ˆà¸™ She gave him a piece of her mind. à¹€à¸˜à¸­à¸•à¸³à¸«à¸™à¸´à¹€à¸‚à¸²à¸•à¸£à¸‡à¹† à¸­à¸¢à¹ˆà¸²à¸‡à¸£à¸¸à¸™à¹à¸£à¸‡
aardwolf	à¸ªà¸±à¸•à¸§à¹Œà¹€à¸¥à¸µà¹‰à¸¢à¸‡à¸¥à¸¹à¸à¸”à¹‰à¸§à¸¢à¸™à¸¡ à¸­à¸­à¸à¸«à¸²à¸à¸´à¸™à¹€à¸§à¸¥à¸²à¸à¸¥à¸²à¸‡à¸„à¸·à¸™ à¸­à¸²à¸¨à¸±à¸¢à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸—à¸µà¹ˆà¸£à¸²à¸šà¸•à¸­à¸™à¹ƒà¸•à¹‰à¸‚à¸­à¸‡à¹à¸­à¸Ÿà¸£à¸´à¸à¸² à¸à¸´à¸™à¸›à¸¥à¸§à¸à¹à¸¥à¸°à¹à¸¡à¸¥à¸‡à¸•à¸±à¸§à¸­à¹ˆà¸­à¸™à¹€à¸›à¹‡à¸™à¸­à¸²à¸«à¸²à¸£ à¹€à¸›à¹‡à¸™à¸ªà¸±à¸•à¸§à¹Œà¸•à¸£à¸°à¸à¸¹à¸¥à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¹„à¸®à¸¢à¸µà¸™à¹ˆà¸²
á€…á€½á€”á€·á€ºá€œá€½á€¾á€á€ºá€§á€€á€›á€¬á€‡á€º	à¸ˆà¸±à¸à¸£à¸à¸£à¸£à¸”à¸´à¹Œà¸œà¸¹à¹‰à¸—à¸µà¹ˆà¸ªà¸¥à¸°à¸£à¸²à¸Šà¸ªà¸¡à¸šà¸±à¸•à¸´
Aberdeen	à¹€à¸¡à¸·à¸­à¸‡à¸—à¹ˆà¸²à¹ƒà¸™à¸•à¸°à¸§à¸±à¸™à¸­à¸­à¸à¹€à¸‰à¸µà¸¢à¸‡à¹€à¸«à¸™à¸·à¸­à¸‚à¸­à¸‡à¸ªà¸à¹‡à¸­à¸•à¹à¸¥à¸™à¸”à¹Œ
á€‘á€„á€ºá€›á€¾á€¬á€¸á€á€±á€¬	à¸—à¸µà¹ˆà¸„à¹ˆà¸­à¸¢à¹†à¸à¸´à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸¡à¹ˆà¸¡à¸¹à¸¡à¸¡à¸²à¸¡, à¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸£à¸°à¸‡à¸±à¸šà¸­à¸²à¸£à¸¡à¸“à¹Œà¸«à¸£à¸·à¸­à¸„à¸§à¸²à¸¡à¸­à¸¢à¸²à¸à¹„à¸”à¹‰à¸”à¸µ à¹€à¸Šà¹ˆà¸™ the longer the intake, the greater the likelihood that a patient will stay continuously abstinent even after termination of alcohol deterrents.
accordian á€á€¶á€á€«á€¸	à¸šà¸²à¸™à¹€à¸Ÿà¸µà¹Šà¸¢à¸¡ à¹€à¸Šà¹ˆà¸™  The accordion door is pleated with many vertical folds and supported by rollers inserted in a track mounted at the top., Related: S.accordion door 
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ tail ./my-th.raw1 
zootomy	à¸à¸²à¸¢à¸§à¸´à¸ à¸²à¸„à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸ªà¸±à¸•à¸§à¹Œ
Zoroaster	à¸œà¸¹à¹‰à¸ªà¸­à¸™à¸¨à¸²à¸ªà¸™à¸²à¸Šà¸²à¸§à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¸µà¸¢
Zoroastrianism	à¸«à¸¥à¸±à¸à¸„à¸§à¸²à¸¡à¹€à¸Šà¸·à¹ˆà¸­à¸‚à¸­à¸‡ Zoroaster
zoster	à¹‚à¸£à¸„à¸‡à¸¹à¸ªà¸§à¸±à¸”
á€‡á€°	à¹€à¸œà¹ˆà¸²à¸‹à¸¹à¸¥à¸¹à¹ƒà¸™à¹à¸­à¸Ÿà¸£à¸´à¸à¸²
á€‡á€°á€¸	à¹€à¸¡à¸·à¸­à¸‡à¸‹à¸¹à¸£à¸´à¸„à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¸ªà¸§à¸´à¸•à¹€à¸‹à¸­à¸£à¹Œà¹à¸¥à¸™à¸”à¹Œ
zwitterion	à¹„à¸­à¸­à¸­à¸™à¸‹à¸¶à¹ˆà¸‡à¸¡à¸µà¸›à¸£à¸°à¸ˆà¸¸à¸šà¸§à¸à¹à¸¥à¸°à¸¥à¸š
zygote	à¹€à¸‹à¸¥à¸¥à¹Œà¸—à¸µà¹ˆà¹€à¸à¸´à¸”à¸ˆà¸²à¸à¸à¸²à¸£à¸£à¸§à¸¡à¸à¸±à¸™à¸‚à¸­à¸‡à¹€à¸‹à¸¥à¸¥à¹Œà¹€à¸à¸¨à¸ªà¸­à¸‡à¹€à¸‹à¸¥à¸¥à¹Œ
zygotic á€–á€¼á€…á€ºá€á€Šá€º	à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š zygote
zymurgy	à¸à¸²à¸£à¸«à¸¡à¸±à¸à¸ªà¸¸à¸£à¸²
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/dict$ 
```

mk-bilingual-dict-train-test.sh á€€á€­á€¯ run á€á€±á€¬á€· error á€•á€±á€¸á€”á€±á€á€²á€·...  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ ./mk-bilingual-dict-train-test.sh /media/ye/project2/exp/bilingual-induction/exp1/dict/my-th.raw1 /media/ye/project2/exp/bilingual-induction/exp1/my/ /media/ye/project2/exp/bilingual-induction/exp1/th/ /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec/ 2>&1 | tee my-th.mk-bi-dict.log
Processing /media/ye/project2/exp/bilingual-induction/exp1/my/3_all.my.word
Processing /media/ye/project2/exp/bilingual-induction/exp1/my/corpus2-and-para
Traceback (most recent call last):
  File "/home/ye/tool/en-cy-bilingual-embeddings/src/get_vocab_from_vectors.py", line 34, in <module>
    w = line.split()[0]
IndexError: list index out of range

real	0m3.662s
user	0m0.632s
sys	0m0.033s
source_vocab.txt
target_vocab.txt
Loading source vocab
Source vocab has 0 words
---
Loading target vocab
Target vocab has 0 words
Loading dictionary - Done 1000 of 125516
Loading dictionary - Done 2000 of 125516
Loading dictionary - Done 3000 of 125516
Loading dictionary - Done 4000 of 125516
...
...
...
Loading dictionary - Done 118000 of 125516
Loading dictionary - Done 119000 of 125516
Loading dictionary - Done 120000 of 125516
Loading dictionary - Done 121000 of 125516
Loading dictionary - Done 122000 of 125516
Loading dictionary - Done 123000 of 125516
Loading dictionary - Done 124000 of 125516
Loading dictionary - Done 125000 of 125516
Loaded 0 entries from the original dictionary, which had: 125516
Traceback (most recent call last):
  File "/home/ye/tool/en-cy-bilingual-embeddings/src/split-dictionary.py", line 54, in <module>
    fdf.columns = ['english', 'welsh']
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/pandas/core/generic.py", line 5500, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas/_libs/properties.pyx", line 70, in pandas._libs.properties.AxisProperty.__set__
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/pandas/core/generic.py", line 766, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/pandas/core/internals/managers.py", line 216, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/pandas/core/internals/base.py", line 57, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 0 elements, new values have 2 elements
>>> exit()

real	0m23.072s
user	0m0.832s
sys	0m1.036s
source_vocab.txt
target_vocab.txt
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$
```

á€¡á€±á€¬á€€á€ºá€•á€« á€¡á€á€½á€²á€€á€¼á€±á€¬á€„á€ºá€·á€œá€¬á€¸?!  

```
zing	à¹‰à¹€à¸à¸´à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸”à¸±à¸‡à¸«à¸§à¸·à¸­à¸œà¹ˆà¸²à¸™à¹„à¸›
```
(vi á€”á€²á€· á€…á€…á€ºá€€á€¼á€Šá€ºá€·á€á€±á€¬á€· TAB á€™á€™á€¼á€„á€ºá€›)

á€™á€†á€­á€¯á€„á€ºá€˜á€°á€¸ á€•á€¼á€¿á€”á€¬á€€ .... mk-bilingual-dict-train-test.sh shell script á€™á€¾á€¬ note á€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€·á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸á€•á€«á€•á€²...   

```
# How to run: ./mk-bilingual-dict-train-test.sh <dictionary> <folder-with-SRC-embeddings> <folder-with-TRG-embeddings> <output-folder-name>
# ./mk-bilingual-dict-train-test.sh /media/ye/project2/exp/bilingual-induction/exp1/dict/my-th.raw1 /media/ye/project2/exp/bilingual-induction/exp1/my/word2vec/ /media/ye/project2/exp/bilingual-induction/exp1/th/word2vec/ /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/ 2>&1 | tee my-th.mk-bi-dict.log
# á€á€á€­á€‘á€¬á€¸á€›á€™á€¾á€¬á€€ $2, $3 argument á€á€½á€±á€€á€­á€¯ á€•á€±á€¸á€á€²á€·á€¡á€á€«á€™á€¾á€¬ folder name á€•á€±á€¸á€›á€™á€šá€ºá‹ á€•á€¼á€®á€¸á€á€±á€¬á€· á€¡á€²á€’á€® folder á€¡á€±á€¬á€€á€ºá€™á€¾á€¬á€€ word2vec á€†á€­á€¯á€›á€„á€ºá€œá€Šá€ºá€¸ word2vec á€–á€­á€¯á€„á€ºá€•á€² á€›á€¾á€­á€á€„á€ºá€·á€á€šá€ºá‹
# á€‘á€­á€¯á€”á€Šá€ºá€¸á€œá€Šá€ºá€¸á€€á€±á€¬á€„á€ºá€¸ fastext á€”á€²á€· bi-lingual induction experiment á€œá€¯á€•á€ºá€á€¬á€†á€­á€¯á€›á€„á€ºá€œá€Šá€ºá€¸ fasttext model á€–á€­á€¯á€„á€ºá€•á€² á€›á€¾á€­á€á€„á€ºá€·á€á€šá€ºá‹ á€•á€±á€¸á€œá€­á€¯á€€á€ºá€á€²á€· folder path á€¡á€±á€¬á€€á€ºá€™á€¾á€¬ á€á€á€¼á€¬á€¸á€–á€­á€¯á€„á€ºá€á€½á€±á€›á€¾á€­á€”á€±á€›á€„á€º error á€•á€±á€¸á€œá€­á€™á€ºá€·á€™á€šá€º
```

á€’á€®á€á€…á€ºá€á€±á€«á€€á€º word2vec folder á€¡á€±á€¬á€€á€ºá€™á€¾á€¬ word2vec á€–á€­á€¯á€„á€º á€á€…á€ºá€–á€­á€¯á€„á€ºá€•á€² á€‘á€¬á€¸á€•á€¼á€®á€¸ run á€á€±á€¬á€· á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ OK á€á€½á€¬á€¸á€á€²á€·...  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ ./mk-bilingual-dict-train-test.sh /media/ye/project2/exp/bilingual-induction/exp1/dict/my-th.raw1 /media/ye/project2/exp/bilingual-induction/exp1/my/word2vec/ /media/ye/project2/exp/bilingual-induction/exp1/th/word2vec/ /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/ 2>&1 | tee my-th.mk-bi-dict.log
Processing /media/ye/project2/exp/bilingual-induction/exp1/my/word2vec/my_corpus.txt_model=word2vec_vectors.vec
Processing /media/ye/project2/exp/bilingual-induction/exp1/th/word2vec/th_corpus.txt_model=word2vec_vectors.vec
Source vocab is 56038 tokens
Target vocab is 28721 tokens

real	0m3.034s
user	0m1.849s
sys	0m0.161s
source_vocab.txt
target_vocab.txt
Loading source vocab
Source vocab has 56038 words
---
Loading target vocab
Target vocab has 28721 words
Loading dictionary - Done 1000 of 125516
Loading dictionary - Done 2000 of 125516
Loading dictionary - Done 3000 of 125516
Loading dictionary - Done 4000 of 125516
Loading dictionary - Done 5000 of 125516
Loading dictionary - Done 6000 of 125516
Loading dictionary - Done 7000 of 125516
Loading dictionary - Done 8000 of 125516
Loading dictionary - Done 9000 of 125516
Loading dictionary - Done 10000 of 125516
Loading dictionary - Done 11000 of 125516
Loading dictionary - Done 12000 of 125516
Loading dictionary - Done 13000 of 125516
Loading dictionary - Done 14000 of 125516
Loading dictionary - Done 15000 of 125516
Loading dictionary - Done 16000 of 125516
Loading dictionary - Done 17000 of 125516
Loading dictionary - Done 18000 of 125516
Loading dictionary - Done 19000 of 125516
Loading dictionary - Done 20000 of 125516
Loading dictionary - Done 21000 of 125516
Loading dictionary - Done 22000 of 125516
Loading dictionary - Done 23000 of 125516
Loading dictionary - Done 24000 of 125516
Loading dictionary - Done 25000 of 125516
Loading dictionary - Done 26000 of 125516
Loading dictionary - Done 27000 of 125516
Loading dictionary - Done 28000 of 125516
Loading dictionary - Done 29000 of 125516
Loading dictionary - Done 30000 of 125516
Loading dictionary - Done 31000 of 125516
Loading dictionary - Done 32000 of 125516
Loading dictionary - Done 33000 of 125516
Loading dictionary - Done 34000 of 125516
Loading dictionary - Done 35000 of 125516
Loading dictionary - Done 36000 of 125516
Loading dictionary - Done 37000 of 125516
Loading dictionary - Done 38000 of 125516
Loading dictionary - Done 39000 of 125516
Loading dictionary - Done 40000 of 125516
Loading dictionary - Done 41000 of 125516
Loading dictionary - Done 42000 of 125516
Loading dictionary - Done 43000 of 125516
Loading dictionary - Done 44000 of 125516
Loading dictionary - Done 45000 of 125516
Loading dictionary - Done 46000 of 125516
Loading dictionary - Done 47000 of 125516
Loading dictionary - Done 48000 of 125516
Loading dictionary - Done 49000 of 125516
Loading dictionary - Done 50000 of 125516
Loading dictionary - Done 51000 of 125516
Loading dictionary - Done 52000 of 125516
Loading dictionary - Done 53000 of 125516
Loading dictionary - Done 54000 of 125516
Loading dictionary - Done 55000 of 125516
Loading dictionary - Done 56000 of 125516
Loading dictionary - Done 57000 of 125516
Loading dictionary - Done 58000 of 125516
Loading dictionary - Done 59000 of 125516
Loading dictionary - Done 60000 of 125516
Loading dictionary - Done 61000 of 125516
Loading dictionary - Done 62000 of 125516
Loading dictionary - Done 63000 of 125516
Loading dictionary - Done 64000 of 125516
Loading dictionary - Done 65000 of 125516
Loading dictionary - Done 66000 of 125516
Loading dictionary - Done 67000 of 125516
Loading dictionary - Done 68000 of 125516
Loading dictionary - Done 69000 of 125516
Loading dictionary - Done 70000 of 125516
Loading dictionary - Done 71000 of 125516
Loading dictionary - Done 72000 of 125516
Loading dictionary - Done 73000 of 125516
Loading dictionary - Done 74000 of 125516
Loading dictionary - Done 75000 of 125516
Loading dictionary - Done 76000 of 125516
Loading dictionary - Done 77000 of 125516
Loading dictionary - Done 78000 of 125516
Loading dictionary - Done 79000 of 125516
Loading dictionary - Done 80000 of 125516
Loading dictionary - Done 81000 of 125516
Loading dictionary - Done 82000 of 125516
Loading dictionary - Done 83000 of 125516
Loading dictionary - Done 84000 of 125516
Loading dictionary - Done 85000 of 125516
Loading dictionary - Done 86000 of 125516
Loading dictionary - Done 87000 of 125516
Loading dictionary - Done 88000 of 125516
Loading dictionary - Done 89000 of 125516
Loading dictionary - Done 90000 of 125516
Loading dictionary - Done 91000 of 125516
Loading dictionary - Done 92000 of 125516
Loading dictionary - Done 93000 of 125516
Loading dictionary - Done 94000 of 125516
Loading dictionary - Done 95000 of 125516
Loading dictionary - Done 96000 of 125516
Loading dictionary - Done 97000 of 125516
Loading dictionary - Done 98000 of 125516
Loading dictionary - Done 99000 of 125516
Loading dictionary - Done 100000 of 125516
Loading dictionary - Done 101000 of 125516
Loading dictionary - Done 102000 of 125516
Loading dictionary - Done 103000 of 125516
Loading dictionary - Done 104000 of 125516
Loading dictionary - Done 105000 of 125516
Loading dictionary - Done 106000 of 125516
Loading dictionary - Done 107000 of 125516
Loading dictionary - Done 108000 of 125516
Loading dictionary - Done 109000 of 125516
Loading dictionary - Done 110000 of 125516
Loading dictionary - Done 111000 of 125516
Loading dictionary - Done 112000 of 125516
Loading dictionary - Done 113000 of 125516
Loading dictionary - Done 114000 of 125516
Loading dictionary - Done 115000 of 125516
Loading dictionary - Done 116000 of 125516
Loading dictionary - Done 117000 of 125516
Loading dictionary - Done 118000 of 125516
Loading dictionary - Done 119000 of 125516
Loading dictionary - Done 120000 of 125516
Loading dictionary - Done 121000 of 125516
Loading dictionary - Done 122000 of 125516
Loading dictionary - Done 123000 of 125516
Loading dictionary - Done 124000 of 125516
Loading dictionary - Done 125000 of 125516
Loaded 14801 entries from the original dictionary, which had: 125516
>>> 

```

á€¡á€‘á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸á€–á€¼á€…á€ºá€”á€±á€›á€„á€º exit() á€”á€²á€· á€‘á€½á€€á€ºá€œá€­á€¯á€€á€ºá€•á€«á‹  


```
>>> exit()

real	4m29.994s
user	0m0.861s
sys	0m1.075s
source_vocab.txt
target_vocab.txt
test_dict.csv
train_dict.csv
```

check the output folder:  

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output$ ls
source_vocab.txt  target_vocab.txt  test_dict.csv  train_dict.csv
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output$ wc *
  56038   56038 1335612 source_vocab.txt
  28721   28721  648843 target_vocab.txt
   2360    4720   97988 test_dict.csv
   9436   18872  390844 train_dict.csv
  96555  108351 2473287 total
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output$ 
```

á€–á€­á€¯á€„á€ºá€¡á€‘á€²á€€ á€…á€¬á€€á€¼á€±á€¬á€„á€ºá€¸á€á€½á€±á€€á€­á€¯á€œá€Šá€ºá€¸ head command á€”á€²á€· á€á€„á€ºá€€á€¼á€Šá€ºá€·á€á€²á€·...  

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output$ head *
==> source_vocab.txt <==
ğŸ¤ 
á€›á€¾á€¬á€œá€„á€ºá€˜á€á€º
á€•á€±á€«á€ºá€œá€½á€„á€º
á€€á€¬á€—á€½á€”á€ºá€’á€­á€¯á€„á€ºá€¡á€±á€¬á€€á€ºá€†á€­á€¯á€€á€º
á€™á€­á€¯á€„á€ºá€¸á€šá€±á€¬
á€¡á€­á€…á€º
á€…á€œá€„á€ºá€¸á€€á€½á€„á€ºá€¸
á€¡á€–á€¼á€°
á€á€€á€ºá€†á€­á€¯á€¸
á€†á€®á€¸á€€á€¼á€­á€¯

==> target_vocab.txt <==
à¸„à¸²à¸à¸±à¸™
à¹€à¸­à¸­à¸£à¹Œà¸§à¸´à¹ˆà¸‡
à¹„à¸à¸£à¸à¸§à¹‰à¸²à¸‡
à¸à¸£à¸¡à¸—à¸
à¹€à¸‹à¸™à¸•à¹Œà¸›à¸µà¹€à¸•à¸­à¸£à¹Œ
à¸ à¸²à¸¢à¹ƒà¸•à¹‰
à¸˜à¸±à¸Š
à¸­à¸à¸¢à¸¸à¸«à¸°à¸„à¸µà¸£à¸µ
à¹€à¸‹à¸­à¸£à¹Œà¹€à¸šà¸µà¸¢
à¸™à¸²à¸¢à¸à¸¥à¹‰à¸²à¸™à¸£à¸‡à¸„à¹Œ

==> test_dict.csv <==
english welsh
á€á€˜á€¬á€ à¸˜à¸²à¸•à¸¸à¹à¸—à¹‰
á€‘á€¯á€•á€ºá€•á€­á€¯á€¸ à¸«à¹ˆà¸­à¸«à¸¸à¹‰à¸¡
á€”á€±á€·á€œá€Šá€º à¹€à¸—à¸µà¹ˆà¸¢à¸‡
á€€á€»á€±á€¬á€·á€€á€½á€„á€ºá€¸ à¸à¸±à¸šà¸”à¸±à¸
á€á€±á€¬á€„á€ºá€•á€­á€¯á€· à¹‚à¸„à¸
á€•á€á€¹á€á€¬ à¸šà¸²à¸™à¸à¸±à¸š
á€á€­á€¯á€„á€ºá€¸á€•á€¼á€Šá€º à¸ à¸¹à¸¡à¸´à¸¥à¸³à¹€à¸™à¸²
á€–á€­á€”á€•á€º à¹€à¸à¸·à¸­à¸
á€™á€¼á€±á€¬á€€á€ºá€˜á€€á€º à¸­à¸¸à¸”à¸£

==> train_dict.csv <==
english welsh
á€¡á€œá€¾á€†á€„á€º à¸›à¸£à¸°à¸”à¸±à¸š
á€šá€²á€·á€šá€²á€· à¹à¸šà¸šà¸šà¸²à¸‡
á€•á€®á€€á€„á€ºá€¸ à¸à¸£à¸¸à¸‡à¸›à¸±à¸à¸à¸´à¹ˆà¸‡
á€•á€­á€¯á€·á€…á€º à¹à¸ˆà¹‰à¸‡
á€á€¶á€á€š à¸à¸´à¸£à¸¸à¸˜
á€€á€¼á€Šá€·á€º à¸ªà¸µà¸«à¸™à¹‰à¸²
á€á€«á€¸ à¹€à¸„à¸µà¹‰à¸¢à¸§
á€á€™á€ºá€¸á€”á€Šá€ºá€¸ à¹‚à¸¨à¸à¸ªà¸¥à¸”
á€›á€¾á€±á€¸á€á€±á€á€º à¹‚à¸šà¸£à¸²à¸“à¸à¸²à¸¥
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output$
```

## Study on VecMap Tool

GitHub Link: [https://github.com/artetxem/vecmap](https://github.com/artetxem/vecmap)  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ python ./vecmap/map_embeddings.py --help
usage: map_embeddings.py [-h] [--encoding ENCODING] [--precision {fp16,fp32,fp64}] [--cuda] [--batch_size BATCH_SIZE] [--seed SEED]
                         [--supervised DICTIONARY | --semi_supervised DICTIONARY | --identical | --unsupervised | --acl2018 | --aaai2018 DICTIONARY | --acl2017 | --acl2017_seed DICTIONARY | --emnlp2016 DICTIONARY]
                         [-d DICTIONARY | --init_identical | --init_numerals | --init_unsupervised] [--unsupervised_vocab UNSUPERVISED_VOCAB]
                         [--normalize [{unit,center,unitdim,centeremb,none} ...]] [--whiten] [--src_reweight [SRC_REWEIGHT]]
                         [--trg_reweight [TRG_REWEIGHT]] [--src_dewhiten {src,trg}] [--trg_dewhiten {src,trg}] [--dim_reduction DIM_REDUCTION]
                         [-c | -u] [--self_learning] [--vocabulary_cutoff VOCABULARY_CUTOFF] [--direction {forward,backward,union}]
                         [--csls [NEIGHBORHOOD_SIZE]] [--threshold THRESHOLD] [--validation DICTIONARY] [--stochastic_initial STOCHASTIC_INITIAL]
                         [--stochastic_multiplier STOCHASTIC_MULTIPLIER] [--stochastic_interval STOCHASTIC_INTERVAL] [--log LOG] [-v]
                         src_input trg_input src_output trg_output

Map word embeddings in two languages into a shared space

positional arguments:
  src_input             the input source embeddings
  trg_input             the input target embeddings
  src_output            the output source embeddings
  trg_output            the output target embeddings

optional arguments:
  -h, --help            show this help message and exit
  --encoding ENCODING   the character encoding for input/output (defaults to utf-8)
  --precision {fp16,fp32,fp64}
                        the floating-point precision (defaults to fp32)
  --cuda                use cuda (requires cupy)
  --batch_size BATCH_SIZE
                        batch size (defaults to 10000); does not affect results, larger is usually faster but uses more memory
  --seed SEED           the random seed (defaults to 0)

recommended settings:
  Recommended settings for different scenarios

  --supervised DICTIONARY
                        recommended if you have a large training dictionary
  --semi_supervised DICTIONARY
                        recommended if you have a small seed dictionary
  --identical           recommended if you have no seed dictionary but can rely on identical words
  --unsupervised        recommended if you have no seed dictionary and do not want to rely on identical words
  --acl2018             reproduce our ACL 2018 system
  --aaai2018 DICTIONARY
                        reproduce our AAAI 2018 system
  --acl2017             reproduce our ACL 2017 system with numeral initialization
  --acl2017_seed DICTIONARY
                        reproduce our ACL 2017 system with a seed dictionary
  --emnlp2016 DICTIONARY
                        reproduce our EMNLP 2016 system

advanced initialization arguments:
  Advanced initialization arguments

  -d DICTIONARY, --init_dictionary DICTIONARY
                        the training dictionary file (defaults to stdin)
  --init_identical      use identical words as the seed dictionary
  --init_numerals       use latin numerals (i.e. words matching [0-9]+) as the seed dictionary
  --init_unsupervised   use unsupervised initialization
  --unsupervised_vocab UNSUPERVISED_VOCAB
                        restrict the vocabulary to the top k entries for unsupervised initialization

advanced mapping arguments:
  Advanced embedding mapping arguments

  --normalize [{unit,center,unitdim,centeremb,none} ...]
                        the normalization actions to perform in order
  --whiten              whiten the embeddings
  --src_reweight [SRC_REWEIGHT]
                        re-weight the source language embeddings
  --trg_reweight [TRG_REWEIGHT]
                        re-weight the target language embeddings
  --src_dewhiten {src,trg}
                        de-whiten the source language embeddings
  --trg_dewhiten {src,trg}
                        de-whiten the target language embeddings
  --dim_reduction DIM_REDUCTION
                        apply dimensionality reduction
  -c, --orthogonal      use orthogonal constrained mapping
  -u, --unconstrained   use unconstrained mapping

advanced self-learning arguments:
  Advanced arguments for self-learning

  --self_learning       enable self-learning
  --vocabulary_cutoff VOCABULARY_CUTOFF
                        restrict the vocabulary to the top k entries
  --direction {forward,backward,union}
                        the direction for dictionary induction (defaults to union)
  --csls [NEIGHBORHOOD_SIZE]
                        use CSLS for dictionary induction
  --threshold THRESHOLD
                        the convergence threshold (defaults to 0.000001)
  --validation DICTIONARY
                        a dictionary file for validation at each iteration
  --stochastic_initial STOCHASTIC_INITIAL
                        initial keep probability stochastic dictionary induction (defaults to 0.1)
  --stochastic_multiplier STOCHASTIC_MULTIPLIER
                        stochastic dictionary induction multiplier (defaults to 2.0)
  --stochastic_interval STOCHASTIC_INTERVAL
                        stochastic dictionary induction interval (defaults to 50)
  --log LOG             write to a log file in tsv format at each iteration
  -v, --verbose         write log information to stderr at each iteration
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ 
```

## Install CuPy Before Running VeccMap

GPU á€á€¯á€¶á€¸á€™á€šá€ºá€†á€­á€¯á€›á€„á€º á€œá€­á€¯á€¡á€•á€ºá€œá€­á€¯á€· á€œá€€á€ºá€›á€¾á€­ environment á€™á€¾á€¬ á€™á€›á€¾á€­á€á€±á€¸á€œá€­á€¯á€·...  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ pip install cupy
Collecting cupy
  Downloading cupy-9.4.0.tar.gz (1.7 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 1.8 MB/s 
Requirement already satisfied: numpy<1.24,>=1.17 in /home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages (from cupy) (1.21.2)
Collecting fastrlock>=0.5
  Using cached fastrlock-0.6-cp39-cp39-manylinux1_x86_64.whl (42 kB)
Building wheels for collected packages: cupy
  Building wheel for cupy (setup.py) ... done
  Created wheel for cupy: filename=cupy-9.4.0-cp39-cp39-linux_x86_64.whl size=59578607 sha256=faf3d463d944b6061c5109e920c524d80ffc3fadf05ca3ec7d48bacdee45809f
  Stored in directory: /home/ye/.cache/pip/wheels/26/46/d7/e279499ab39f81388e4b29f0f8d335b5253aa791b68dced02e
Successfully built cupy
Installing collected packages: fastrlock, cupy
Successfully installed cupy-9.4.0 fastrlock-0.6
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ 
```

--cuda option á€”á€²á€· run á€á€²á€·á€¡á€á€«á€™á€¾á€¬ memory error á€†á€­á€¯á€•á€¼á€®á€¸á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ ERROR!   

```
Traceback (most recent call last):
  File "/home/ye/tool/en-cy-bilingual-embeddings/./vecmap/map_embeddings.py", line 422, in <module>
    main()
  File "/home/ye/tool/en-cy-bilingual-embeddings/./vecmap/map_embeddings.py", line 349, in main
    dropout(simfwd[:j-i], 1 - keep_prob).argmax(axis=1, out=trg_indices_forward[i:j])
  File "/home/ye/tool/en-cy-bilingual-embeddings/./vecmap/map_embeddings.py", line 32, in dropout
    mask = xp.random.rand(*m.shape) >= p
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/cupy/random/_sample.py", line 44, in rand
    return random_sample(size=size, dtype=dtype)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/cupy/random/_sample.py", line 156, in random_sample
    return rs.random_sample(size=size, dtype=dtype)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/cupy/random/_generator.py", line 618, in random_sample
    out = self._random_sample_raw(size, dtype)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/cupy/random/_generator.py", line 600, in _random_sample_raw
    out = cupy.empty(size, dtype=dtype)
  File "/home/ye/anaconda3/envs/bilingual-emb/lib/python3.9/site-packages/cupy/_creation/basic.py", line 22, in empty
    return cupy.ndarray(shape, dtype, order=order)
  File "cupy/_core/core.pyx", line 163, in cupy._core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 718, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1395, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1416, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1096, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1117, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1355, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 1,600,000,000 bytes (allocated so far: 2,281,307,136 bytes).

real	0m5.425s
user	0m5.231s
sys	0m1.389s
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$
```

á€¡á€²á€’á€«á€€á€¼á€±á€¬á€„á€ºá€· --cuda option á€€á€­á€¯ á€–á€¼á€¯á€á€ºá€•á€¼á€®á€¸ run á€á€²á€·á€á€šá€º...  
Running time á€€ semi á€€á€…á€•á€¼á€®á€¸ á€€á€¼á€¬á€œá€­á€™á€ºá€·á€™á€šá€ºá‹  

## word2vec filenaming

á€œá€€á€ºá€›á€¾á€­ word2vec á€€á€­á€¯ training á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€¬á€€ á€¡á€±á€¬á€€á€ºá€•á€« settting á€”á€²á€·  
(see train_embeddings.py python script)  

```python
	model = modelmap[args.model](size=300, window=5, min_count=3, sentences=corpus, iter=10)
```

á€¡á€²á€’á€«á€€á€¼á€±á€¬á€„á€ºá€· á€†á€±á€¬á€€á€ºá€‘á€¬á€¸á€á€²á€· word2vec model á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€· format á€¡á€á€­á€¯á€„á€ºá€¸ filename á€€á€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸á€á€²á€·...  

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super$ mv src_mapped_supervised.emb word2vec_s300_mc3_w5.vec
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super$ cd ../trg_super/
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super$ mv trg_mapped_supervised.emb word2vec_s300_mc3_w5.vec
```

## Dictionary Induction and Evaluation

á€’á€® log á€™á€™á€¾á€á€ºá€á€„á€º á€•á€‘á€™á€†á€¯á€¶á€¸ run á€€á€¼á€Šá€ºá€·á€á€¯á€”á€ºá€¸á€€ á€•á€±á€¸á€á€²á€· error á€€ á€–á€­á€¯á€„á€ºá€”á€¬á€™á€Šá€ºá€€á€­á€¯ á€á€°á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸ á€™á€•á€±á€¸á€á€²á€·á€œá€­á€¯á€·á€†á€­á€¯á€á€¬ coding á€á€„á€ºá€–á€á€ºá€›á€„á€ºá€¸ á€á€­á€á€²á€·á€›á€œá€­á€¯á€·...   
á€’á€®á€á€…á€ºá€á€«á€á€±á€¬á€· á€–á€­á€¯á€„á€ºá€”á€¬á€™á€Šá€ºá€á€½á€±á€€á€­á€¯ \_s, \_mc, \_w á€”á€±á€¬á€€á€ºá€™á€¾á€¬ parameter á€á€½á€±á€€á€­á€¯ á€‘á€Šá€ºá€·á€•á€±á€¸á€á€¬á€œá€¯á€•á€ºá€á€²á€·...  

á€œá€±á€¬á€œá€±á€¬á€†á€šá€ºá€€ á€á€°á€· default parameter á€á€½á€±á€”á€²á€·á€•á€² word2vec á€™á€±á€¬á€ºá€’á€šá€ºá€€á€­á€¯ á€†á€±á€¬á€€á€ºá€á€²á€·á€á€¬á€™á€­á€¯á€·...  

```
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ ./experiment1.sh /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/train_dict.csv /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/ /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/  /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/test_dict.csv /media/ye/project2/exp/bilingual-induction/exp1/my-th/induction/
run vecmap_launcher.py ...
path:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec
path:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec
EN:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec
WEL:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec
=== CMD ===
python3 vecmap/map_embeddings.py --supervised /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/train_dict.csv "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec"
WARNING: OOV dictionary entry (english - welsh)
run vecmap_eval_launcher.py ...

arguments:  Namespace(testdict='/media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/test_dict.csv', source_vectors_folder='/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/', target_vectors_folder='/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/', results_folder='/media/ye/project2/exp/bilingual-induction/exp1/my-th/induction/')
envecs:  ['/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec']
welvecs:  ['/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec']
env:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec
enprefix:  word2vec_s ens:  300 enmc:  3 enw:  5
EN:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec
WEL:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec
RETR:  nn
output result folder: /media/ye/project2/exp/bilingual-induction/exp1/my-th/induction/
=== CMD ===
python3 vecmap/eval_translation.py "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec" -d /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/test_dict.csv --retrieval nn
--------
EN:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec
WEL:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec
RETR:  invsoftmax
output result folder: /media/ye/project2/exp/bilingual-induction/exp1/my-th/induction/
=== CMD ===
python3 vecmap/eval_translation.py "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec" -d /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/test_dict.csv --retrieval invsoftmax
--------
EN:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec
WEL:  /media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec
RETR:  csls
output result folder: /media/ye/project2/exp/bilingual-induction/exp1/my-th/induction/
=== CMD ===
python3 vecmap/eval_translation.py "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/src_super/word2vec_s300_mc3_w5.vec_mapped.vec" "/media/ye/project2/exp/bilingual-induction/exp1/my-th/vecmap-output/trg_super/word2vec_s300_mc3_w5.vec_mapped.vec" -d /media/ye/project2/exp/bilingual-induction/exp1/my-th/word2vec-output/test_dict.csv --retrieval csls
--------
(bilingual-emb) ye@:~/tool/en-cy-bilingual-embeddings$ 
```

```
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/induction$ ls
'model=word2vec_s__retrieval=csls__s=300_mc=3_w=5.txt'        'model=word2vec_s__retrieval=nn__s=300_mc=3_w=5.txt'
'model=word2vec_s__retrieval=invsoftmax__s=300_mc=3_w=5.txt'
(base) ye@:/media/ye/project2/exp/bilingual-induction/exp1/my-th/induction$ cat *
Coverage: 99.94%  Accuracy:  2.50%
Coverage: 99.94%  Accuracy:  2.32%
Coverage: 99.94%  Accuracy:  2.44%
```

Running Process á€¡á€…á€¡á€†á€¯á€¶á€¸á€á€±á€¬á€· á€›á€á€½á€¬á€¸á€•á€¼á€®á‹  

## To Do

- Word2Vec á€™á€±á€¬á€ºá€’á€šá€ºá€€á€­á€¯ parameter á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸ á€‘á€¬á€¸ model á€†á€±á€¬á€€á€ºá€•á€¼á€®á€¸ evaluation á€œá€¯á€•á€ºá€€á€¼á€Šá€ºá€·á€›á€”á€º
- fasttext á€™á€±á€¬á€ºá€’á€šá€ºá€”á€²á€·á€œá€Šá€ºá€¸ embedding á€œá€¯á€•á€ºá€•á€¼á€®á€¸ induction á€€á€­á€¯ evaluation á€œá€¯á€•á€ºá€€á€¼á€Šá€ºá€·á€›á€”á€º
- á€¡á€á€»á€­á€”á€ºá€›á€›á€„á€º VecMap mapping á€€á€­á€¯á€œá€Šá€ºá€¸ Supervised, Semi-Supervised, Identical á€”á€²á€· Unsupervised á€¡á€¬á€¸á€œá€¯á€¶á€¸á€”á€²á€· experiment á€œá€¯á€•á€ºá€€á€¼á€Šá€ºá€·á€›á€”á€º


## Reference

https://github.com/marekrei/convertvec

Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?:  
https://ie.technion.ac.il/~roiri/papers/EMNLP-Ivan-CLWE.pdf

PanLex-based bilingual lexicons for 210 language pairs (15 languages):  
https://github.com/ivulic/panlex-bli

Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings:  
http://uu.diva-portal.org/smash/get/diva2:1365879/FULLTEXT01.pdf

Cross-lingual word and document embeddings:  
https://www.youtube.com/watch?v=2a-D7L8rdko

How to (Properly) Evaluate Cross-Lingual Word Embeddings:
On Strong Baselines, Comparative Analyses, and Some Misconceptions
https://aclanthology.org/P19-1070.pdf

Lost in Embedding Space: Explaining Cross-Lingual Task Performance with Eigenvalue Divergence:  
https://arxiv.org/pdf/2001.11136v1.pdf


